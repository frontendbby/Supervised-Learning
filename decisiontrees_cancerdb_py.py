# -*- coding: utf-8 -*-
"""DecisionTrees_CancerDB_py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/jessicaromero-ctrl/Advanced-Math-for-ML/blob/main/DecisionTrees_CancerDB_py.ipynb
"""

from sklearn import datasets

from sklearn import tree
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

dataset = datasets.load_breast_cancer()
print(dataset)

import pandas as pd
from sklearn.datasets import load_breast_cancer

# 1. Cargar el dataset
cancer = load_breast_cancer()

# 2. Crear un DataFrame de pandas con los datos y nombres de columnas
df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)

# 3. Agregar la columna objetivo (target)
df['target'] = cancer.target

# 4. Mostrar las primeras filas
#print(df.head())
df.head

# Para ver la descripción general
#print(df.info())

print("Información en el Dataset: ")
print(dataset.keys())
print

#print("Características del Dataset: ")
#print(dataset.DESCR())

X = dataset.data
y = dataset.target

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier # Added this import

#Separación de los datos
X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2)

#Se escalan los datos
#from sklearn.preprocessing import StandardScaler
escalar = StandardScaler()
X_train = escalar.fit_transform(X_train)
X_test = escalar.transform(X_test)

#Se define el algoritmo a utilizar
algoritmo = DecisionTreeClassifier()

#Entrenamos el modelo
algoritmo.fit(X_train, y_train);

#Se realiza la predicción
y_pred = algoritmo.predict(X_test)

#Se realiza la probabilidad
y_proba = algoritmo.predict_proba(X_test)[:, 1]

#Se verifica con la matriz de confusión
from sklearn.metrics import confusion_matrix
matriz = confusion_matrix(y_test, y_pred)
print("Matriz de confusión: ")
print(matriz)

#Se calcula la precisión del modelo
from sklearn.metrics import precision_score

precision = precision_score(y_test,y_pred)
print("Precisión del Modelo: ")
print(precision)

#Calculamos la exactitud del modelo
from sklearn.metrics import accuracy_score

exactitud = accuracy_score(y_test,y_pred)
print("Exactitud del Modelo: ")
print(exactitud)

#Calculamos la sensibilidad del modelo
from sklearn.metrics import recall_score

sensibilidad = recall_score(y_test,y_pred)
print("Sensibilidad del Modelo: ")
print(sensibilidad)

#Calculamos el puntaje F1 del modelo
from sklearn.metrics import f1_score

puntajeF1 = f1_score(y_test,y_pred)
print("Puntaje F1 del Modelo: ")
print(puntajeF1)

#Calculamos la curva ROC-AUC del modelo
from sklearn.metrics import roc_auc_score

#roc_auc = roc_auc_score(y_test,y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
print("Curva ROC - AUC del Modelo: ")
print(roc_auc)

#print("Precisión del Modelo: ", precision)
#print("Exactitud del Modelo: ",exactitud)
#print("Sensibilidad del Modelo: ",sensibilidad)
#print("Puntaje F1 del Modelo: ", puntajeF1)
#print("Curva ROC - AUC del Modelo: ", roc_auc)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# 1. Obtener probabilidades de la clase positiva ***PUEDE OMITIRSE PORQUE SE OBTUVO PREVIAMENTE***
#y_proba = algoritmo.predict_proba(X_test)[:, 1]

# 2. Calcular FPR, TPR y umbrales
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# 3. Calcular el área bajo la curva (AUC)
roc_auc = auc(fpr, tpr)

# 4. Graficar
plt.figure()
plt.plot(fpr, tpr, label=f"Curva ROC (AUC = {roc_auc:.3f})")
plt.plot([0, 1], [0, 1], linestyle="--", label="Clasificador aleatorio")

plt.xlabel("Tasa de Falsos Positivos (FPR)")
plt.ylabel("Tasa de Verdaderos Positivos (TPR)")
plt.title("Curva ROC - Regresión Logística")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""## Resultados:
Matriz de confusión:

[[44  3]
 [ 5 62]]

This matrix indicates that the model **correctly classified 44 negative cases (true negatives) and 62 positive cases (true positives).** It misclassified 3 negative cases as positive (false positives) and 5 positive cases as negative (false negatives).

Precisión del Modelo (Precision):** 0.954 This means that when the model predicted a positive outcome, it was correct approximately 95.4% of the time.**

Exactitud del Modelo (Accuracy): **0.930 The model correctly classified about 93.0% of all the samples.**

Sensibilidad del Modelo (Recall): **0.925 This indicates that the model correctly identified 92.5% of all actual positive cases.**

Puntaje F1 del Modelo (F1 Score):** 0.939 The F1 Score provides a balance between precision and recall, with a value of 0.939 suggesting a good overall performance, especially useful when there might be an uneven class distribution.**

Curva ROC - AUC del Modelo (ROC AUC Score):** 0.931 An AUC score of 0.931 suggests that the model has a very good ability to distinguish between the positive and negative classes.**
"""