# -*- coding: utf-8 -*-
"""GaussianNB_py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/jessicaromero-ctrl/Advanced-Math-for-ML/blob/main/GaussianNB_py.ipynb
"""

from sklearn import datasets

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

dataset = datasets.load_breast_cancer()
print(dataset)

import pandas as pd
from sklearn.datasets import load_breast_cancer

# 1. Cargar el dataset
cancer = load_breast_cancer()

# 2. Crear un DataFrame de pandas con los datos y nombres de columnas
df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)

# 3. Agregar la columna objetivo (target)
df['target'] = cancer.target

# 4. Mostrar las primeras filas
#print(df.head())
df.head

# Para ver la descripción general
#print(df.info())

print("Información en el Dataset: ")
print(dataset.keys())
print

#print("Características del Dataset: ")
#print(dataset.DESCR())

X = dataset.data
y = dataset.target

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#Separación de los datos
X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2)

#Se escalan los datos
#from sklearn.preprocessing import StandardScaler
escalar = StandardScaler()
X_train = escalar.fit_transform(X_train)
X_test = escalar.transform(X_test)

#Se define el algoritmo a utilizar
# The previous cell (fw1acpS5zUgE) already imports GaussianNB from sklearn.naive_bayes
algoritmo = GaussianNB()

#Entrenamos el modelo
algoritmo.fit(X_train, y_train)

#Se realiza la predicción
y_pred = algoritmo.predict(X_test)

#Se realiza la probabilidad
y_proba = algoritmo.predict_proba(X_test)[:, 1]

#Se verifica con la matriz de confusión
from sklearn.metrics import confusion_matrix
matriz = confusion_matrix(y_test, y_pred)
print("Matriz de confusión: ")
print(matriz)

#Se calcula la precisión del modelo
from sklearn.metrics import precision_score

precision = precision_score(y_test,y_pred)
print("Precisión del Modelo: ")
print(precision)

#Calculamos la exactitud del modelo
from sklearn.metrics import accuracy_score

exactitud = accuracy_score(y_test,y_pred)
print("Exactitud del Modelo: ")
print(exactitud)

#Calculamos la sensibilidad del modelo
from sklearn.metrics import recall_score

sensibilidad = recall_score(y_test,y_pred)
print("Sensibilidad del Modelo: ")
print(sensibilidad)

#Calculamos el puntaje F1 del modelo
from sklearn.metrics import f1_score

puntajeF1 = f1_score(y_test,y_pred)
print("Puntaje F1 del Modelo: ")
print(puntajeF1)

#Calculamos la curva ROC-AUC del modelo
from sklearn.metrics import roc_auc_score

#roc_auc = roc_auc_score(y_test,y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
print("Curva ROC - AUC del Modelo: ")
print(roc_auc)

#print("Precisión del Modelo: ", precision)
#print("Exactitud del Modelo: ",exactitud)
#print("Sensibilidad del Modelo: ",sensibilidad)
#print("Puntaje F1 del Modelo: ", puntajeF1)
#print("Curva ROC - AUC del Modelo: ", roc_auc)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# 1. Obtener probabilidades de la clase positiva ***PUEDE OMITIRSE PORQUE SE OBTUVO PREVIAMENTE***
#y_proba = algoritmo.predict_proba(X_test)[:, 1]

# 2. Calcular FPR, TPR y umbrales
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# 3. Calcular el área bajo la curva (AUC)
roc_auc = auc(fpr, tpr)

# 4. Graficar
plt.figure()
plt.plot(fpr, tpr, label=f"Curva ROC (AUC = {roc_auc:.3f})")
plt.plot([0, 1], [0, 1], linestyle="--", label="Clasificador aleatorio")

plt.xlabel("Tasa de Falsos Positivos (FPR)")
plt.ylabel("Tasa de Verdaderos Positivos (TPR)")
plt.title("Curva ROC - Regresión Logística")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""## Resultados:

Matriz de confusión:

 | [[45  4]
 | [ 0 65]]

This matrix shows that **the model correctly predicted 45 negative cases (true negatives) and 65 positive cases (true positives).** It incorrectly predicted 4 negative cases as positive (false positives) and 0 positive cases as negative (false negatives).

Precisión del Modelo (Precision): 0.942 Precision is the proportion of true positive predictions among all positive predictions. In this case, **94.2% of the cases predicted as positive were actually positive.**

Exactitud del Modelo (Accuracy): 0.965 Accuracy is the proportion of correctly classified instances over the total number of instances. **The model correctly classified approximately 96.5% of the samples.**

Sensibilidad del Modelo (Recall): **1.0 Recall is the proportion of true positive predictions among all actual positive cases.** A recall of 1.0 (100%) means the model identified all actual positive cases correctly (no false negatives).

Puntaje F1 del Modelo (F1 Score): 0.970 The F1 Score is the harmonic mean of precision and recall. It's a good metric when you need a **balance between precision and recall, especially with imbalanced classes.** A score of 0.970 indicates a very good balance.

Curva ROC - AUC del Modelo (ROC AUC Score): 0.997 The Area Under the Receiver Operating Characteristic Curve (ROC AUC) **measures the model's ability to distinguish between classes.** An AUC of 0.997 is very close to 1, indicating an excellent **ability of the model to separate the positive and negative classes.**
"""